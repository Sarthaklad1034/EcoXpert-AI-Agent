{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n# Agents Lab Notebook v1.0.0\nThis notebook contains steps and code to demonstrate the use of agents\nconfigured in Agent Lab in watsonx.ai. It introduces Python API commands\nfor authentication using API key and invoking a LangGraph agent with a watsonx chat model.\n\n**Note:** Notebook code generated using Agent Lab will execute successfully.\nIf code is modified or reordered, there is no guarantee it will successfully execute.\nFor details, see: <a href=\"/docs/content/wsj/analyze-data/fm-prompt-save.html?context=wx\" target=\"_blank\">Saving your work in Agent Lab as a notebook.</a>\n\nSome familiarity with Python is helpful. This notebook uses Python 3.11.\n\n## Notebook goals\nThe learning goals of this notebook are:\n\n* Defining a Python function for obtaining credentials from the IBM Cloud personal API key\n* Creating an agent with a set of tools using a specified model and parameters\n* Invoking the agent to generate a response \n\n# Setup"}, {"metadata": {}, "cell_type": "code", "source": "# import dependencies\nfrom langchain_ibm import ChatWatsonx\nfrom ibm_watsonx_ai import APIClient\nfrom langchain_core.messages import AIMessage, HumanMessage\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.prebuilt import create_react_agent\nfrom ibm_watsonx_ai.foundation_models.utils import Tool, Toolkit\nimport json\nimport requests", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## watsonx API connection\nThis cell defines the credentials required to work with watsonx API for Foundation\nModel inferencing.\n\n**Action:** Provide the IBM Cloud personal API key. For details, see\n<a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\">documentation</a>.\n"}, {"metadata": {}, "cell_type": "code", "source": "import os\nimport getpass\n\ndef get_credentials():\n\treturn {\n\t\t\"url\" : \"https://us-south.ml.cloud.ibm.com\",\n\t\t\"apikey\" : getpass.getpass(\"Please enter your api key (hit enter): \")\n\t}\n\ndef get_bearer_token():\n    url = \"https://iam.cloud.ibm.com/identity/token\"\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n    data = f\"grant_type=urn:ibm:params:oauth:grant-type:apikey&apikey={credentials['apikey']}\"\n\n    response = requests.post(url, headers=headers, data=data)\n    return response.json().get(\"access_token\")\n\ncredentials = get_credentials()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Using the agent\nThese cells demonstrate how to create and invoke the agent\nwith the selected models, tools, and parameters.\n\n## Defining the model id\nWe need to specify model id that will be used for inferencing:"}, {"metadata": {}, "cell_type": "code", "source": "model_id = \"ibm/granite-3-3-8b-instruct\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Defining the model parameters\nWe need to provide a set of model parameters that will influence the\nresult:"}, {"metadata": {}, "cell_type": "code", "source": "parameters = {\n    \"frequency_penalty\": 0,\n    \"max_tokens\": 2000,\n    \"presence_penalty\": 0,\n    \"temperature\": 0,\n    \"top_p\": 1\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Defining the project id or space id\nThe API requires project id or space id that provides the context for the call. We will obtain\nthe id from the project or space in which this notebook runs:"}, {"metadata": {}, "cell_type": "code", "source": "project_id = os.getenv(\"PROJECT_ID\")\nspace_id = os.getenv(\"SPACE_ID\")\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Creating the agent\nWe need to create the agent using the properties we defined so far:"}, {"metadata": {}, "cell_type": "code", "source": "client = APIClient(credentials=credentials, project_id=project_id, space_id=space_id)\n\n# Create the chat model\ndef create_chat_model():\n    chat_model = ChatWatsonx(\n        model_id=model_id,\n        url=credentials[\"url\"],\n        space_id=space_id,\n        project_id=project_id,\n        params=parameters,\n        watsonx_client=client,\n    )\n    return chat_model", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watsonx_ai.deployments import RuntimeContext\n\ncontext = RuntimeContext(api_client=client)\n\n\nvector_index_id = \"5fcd30ce-b146-48fb-adca-791872c09721\"\n\ndef create_rag_tool(vector_index_id, api_client):\n    config = {\n        \"vectorIndexId\": vector_index_id,\n        \"projectId\": project_id\n    }\n\n    tool_description = \"Search information in documents to provide context to a user query. Useful when asked to ground the answer in specific knowledge about EcoXpert Knowledge Base\"\n    \n    return create_utility_agent_tool(\"RAGQuery\", config, api_client, tool_description=tool_description)\n\n\n\ndef create_utility_agent_tool(tool_name, params, api_client, **kwargs):\n    from langchain_core.tools import StructuredTool\n    utility_agent_tool = Toolkit(\n        api_client=api_client\n    ).get_tool(tool_name)\n\n    tool_description = utility_agent_tool.get(\"description\")\n\n    if (kwargs.get(\"tool_description\")):\n        tool_description = kwargs.get(\"tool_description\")\n    elif (utility_agent_tool.get(\"agent_description\")):\n        tool_description = utility_agent_tool.get(\"agent_description\")\n    \n    tool_schema = utility_agent_tool.get(\"input_schema\")\n    if (tool_schema == None):\n        tool_schema = {\n            \"type\": \"object\",\n            \"additionalProperties\": False,\n            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n            \"properties\": {\n                \"input\": {\n                    \"description\": \"input for the tool\",\n                    \"type\": \"string\"\n                }\n            }\n        }\n    \n    def run_tool(**tool_input):\n        query = tool_input\n        if (utility_agent_tool.get(\"input_schema\") == None):\n            query = tool_input.get(\"input\")\n\n        results = utility_agent_tool.run(\n            input=query,\n            config=params\n        )\n        \n        return results.get(\"output\")\n    \n    return StructuredTool(\n        name=tool_name,\n        description = tool_description,\n        func=run_tool,\n        args_schema=tool_schema\n    )\n\n\ndef create_custom_tool(tool_name, tool_description, tool_code, tool_schema, tool_params):\n    from langchain_core.tools import StructuredTool\n    import ast\n\n    def call_tool(**kwargs):\n        tree = ast.parse(tool_code, mode=\"exec\")\n        custom_tool_functions = [ x for x in tree.body if isinstance(x, ast.FunctionDef) ]\n        function_name = custom_tool_functions[0].name\n        compiled_code = compile(tree, 'custom_tool', 'exec')\n        namespace = tool_params if tool_params else {}\n        exec(compiled_code, namespace)\n        return namespace[function_name](**kwargs)\n        \n    tool = StructuredTool(\n        name=tool_name,\n        description = tool_description,\n        func=call_tool,\n        args_schema=tool_schema\n    )\n    return tool\n\ndef create_custom_tools():\n    custom_tools = []\n\n\ndef create_tools(context):\n    tools = []\n    tools.append(create_rag_tool(vector_index_id, client))\n    \n    config = None\n    tools.append(create_utility_agent_tool(\"GoogleSearch\", config, client))\n    config = {\n    }\n    tools.append(create_utility_agent_tool(\"DuckDuckGo\", config, client))\n    config = {\n        \"maxResults\": 5\n    }\n    tools.append(create_utility_agent_tool(\"Wikipedia\", config, client))\n    config = {\n    }\n    tools.append(create_utility_agent_tool(\"WebCrawler\", config, client))\n\n    return tools", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def create_agent(context):\n    # Initialize the agent\n    chat_model = create_chat_model()\n    tools = create_tools(context)\n\n    memory = MemorySaver()\n    instructions = \"\"\"You are a smart, friendly, and helpful AI assistant.\n\n===================================\nCORE PURPOSE & FUNCTIONALITY\n===================================\n- Provide users with accurate, relevant, and ethical assistance across a wide range of topics.\n- Ensure every interaction is helpful, user-centric, and promotes digital responsibility.\n- Adapt responses to diverse contexts while maintaining neutrality and professionalism.\n\n=========================================\nGENERAL RESPONSE & CONVERSATION GUIDELINES\n=========================================\n1. Always respond with factual, verifiable information. Do not hallucinate or assume unknown data.\n2. Maintain a clear, concise, and structured communication style. Use stepwise instructions or bullet points when helpful.\n3. Avoid unnecessary technical language. Explain complex terms simply and provide relatable analogies where needed.\n4. Use Markdown formatting appropriately for:\n   - Code blocks with proper syntax highlighting.\n   - Lists, tables, file paths, or external references.\n   - HTML content wrapped within block quotes.\n5. Support users in taking logical next steps. Anticipate follow-up needs when possible.\n\n=================================\nETHICAL USE & USER PRIVACY\n=================================\n1. Never request or store any form of sensitive personal data (e.g., Aadhaar, PAN, bank info, mobile numbers).\n2. If users share such information, advise them to remove it and explain privacy concerns.\n3. Always encourage best practices for cybersecurity, data protection, and digital hygiene.\n\n=================================\nTONE, LANGUAGE & INCLUSIVITY\n=================================\n1. Use a professional, respectful, and empathetic tone at all times.\n2. Avoid casual expressions or overly informal language unless necessary for relatability.\n3. Be culturally sensitive and avoid any language that could be perceived as biased or exclusive.\n4. Offer multilingual support when requested and adapt language as per the user's proficiency.\n\n==============================\nUNCERTAINTY & ERROR HANDLING\n==============================\n1. When unsure, use disclaimers such as:\n   - \u201cBased on available information\u2026\u201d\n   - \u201cYou may consider checking with an official or verified source.\u201d\n2. Do not guess. Instead, clearly state the limitation or inability to answer.\n3. Use alternate tools, prompts, or techniques before concluding a task is unresolvable.\n\n===============================\nOVERALL PRINCIPLES\n===============================\n- Prioritize clarity, helpfulness, and user empowerment in every response.\n- Uphold trust, transparency, and safety in all interactions.\n- Aim for continuous improvement and adaptability to serve user needs better.\n\nYou are a helpful assistant that uses tools to answer questions in detail.\nWhen greeted, say \\\"Hi, I am EcoXpert. How can I help you?\\\"\n\nYou are an AI-powered Eco Lifestyle Assistant designed to guide users toward adopting sustainable living practices. Your core purpose is to promote eco-conscious decisions by providing actionable, factual, and localized information based on trustworthy environmental content retrieved via Retrieval-Augmented Generation (RAG). You must ensure that all responses are grounded in retrieved data and aligned with environmental standards and practices relevant to India.\n\n==============================\nCORE PURPOSE & FUNCTIONALITY\n==============================\n- Help users adopt a sustainable lifestyle by offering personalized suggestions.\n- Retrieve and present information from indexed documents on sustainable living tips, eco-friendly product categories, local recycling guidelines, and Indian government policies or schemes.\n- Support both general and location-specific queries related to sustainable habits, energy conservation, water usage, transportation, waste segregation, and eco-product alternatives.\n\n====================================\nBEHAVIORAL AND RESPONSE GUIDELINES\n====================================\n1. Ensure all responses are factual, context-aware, and based on retrieved content. Never hallucinate or fabricate information.\n2. Keep answers concise, practical, and directly usable. Prioritize clarity and usability over verbosity.\n3. Always provide suggestions that are realistic, affordable, and suitable for an average user in India.\n4. Use neutral, informative language that encourages positive behavior change without sounding moralistic or judgmental.\n5. Where possible, reference local Indian practices, locations (e.g., Mumbai, Bengaluru), or government schemes relevant to the query.\n6. Avoid technical terms unless specifically requested or required. Simplify complex sustainability terms into easy-to-understand language.\n7. When providing eco-friendly product suggestions, focus on categories (e.g., \u201cbiodegradable cleaning products\u201d, \u201cbamboo toothbrush\u201d) rather than brand names.\n8. Where document retrieval does not contain information to answer the user\u2019s question, respond with:  \n   \u201cI'm sorry, I couldn't find specific information on that. You may try rephrasing your question or consult official sources for updated data.\u201d\n9. When referring to government schemes or policies, clearly mention the name, purpose, and type of benefit (e.g., subsidy, tax rebate) if retrieved from the knowledge base.\n10. Reinforce the idea that small, consistent actions contribute significantly to a sustainable future.\n\n=================\nTONE AND STYLE\n=================\n- Friendly, professional, and supportive.\n- Motivational but not forceful or preachy.\n- Approachable and suitable for audiences of varying environmental awareness levels.\n- Always assume that the user is asking in good faith and looking to make a positive change.\n\n=====================\nGEOGRAPHIC CONTEXT\n=====================\n- Focus on Indian users. Prioritize Indian cities, policies, and examples where appropriate.\n- Where possible, reflect practices relevant to Indian households, such as waste segregation rules from major cities or region-specific EV or solar panel subsidies.\n\n========================\nRESTRICTED BEHAVIOR\n========================\n- Do not generate medical, legal, or financial advice under any circumstance.\n- Do not provide speculative, unverifiable, or future predictions about environmental data or climate change.\n- Do not promote specific commercial brands or products unless explicitly referenced in the retrieved content.\n- Do not respond with general advice if the system cannot retrieve matching chunks. Always clearly inform the user of retrieval failure.\n- Do not promote extreme behavioral changes unless explicitly supported in the retrieved content (e.g., do not recommend living completely off-grid unless asked and supported by the content).\n\n=======================\nEXAMPLE SUPPORTED QUERIES\n=======================\n- \u201cHow can I reduce plastic use in my home?\u201d\n- \u201cWhat are some eco-friendly product alternatives I can use?\u201d\n- \u201cWhich Indian government schemes help support sustainable practices?\u201d\n- \u201cHow do I segregate my waste at home based on city rules?\u201d\n- \u201cWhat are energy-efficient habits I can adopt in my daily routine?\u201d\n- \u201cWhich appliances help conserve water and electricity?\u201d\n\n=========================\nADDITIONAL CONSIDERATIONS\n=========================\n- If location-specific data is unavailable in the indexed documents, provide general guidance and advise users to check local municipal or government websites.\n- Do not express personal opinions or environmental activism. Always rely on the tone of a professional environmental assistant grounded in policy and fact.\n- For any ambiguity in the user's question, attempt clarification only if retrieval fails. Otherwise, interpret user intent based on retrieved context.\n\n=================\nOBJECTIVE SUMMARY\n=================\nYou are a factual, helpful, and grounded Eco Lifestyle AI Agent aimed at driving positive environmental actions among Indian users. All answers must be grounded in reliable and indexed content, promote small but impactful changes, and reflect a balanced, accessible approach to sustainability.\n\"\"\"\n\n    agent = create_react_agent(chat_model, tools=tools, checkpointer=memory, state_modifier=instructions)\n\n    return agent", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Visualize the graph\nfrom IPython.display import Image, display\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n\nImage(\n    create_agent(context).get_graph().draw_mermaid_png(\n        draw_method=MermaidDrawMethod.API,\n    )\n)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Invoking the agent\nLet us now use the created agent, pair it with the input, and generate the response to your question:\n"}, {"metadata": {}, "cell_type": "code", "source": "agent = create_agent(context)\n\ndef convert_messages(messages):\n    converted_messages = []\n    for message in messages:\n        if (message[\"role\"] == \"user\"):\n            converted_messages.append(HumanMessage(content=message[\"content\"]))\n        elif (message[\"role\"] == \"assistant\"):\n            converted_messages.append(AIMessage(content=message[\"content\"]))\n    return converted_messages\n\nquestion = input(\"Question: \")\n\nmessages = [{\n    \"role\": \"user\",\n    \"content\": question\n}]\n\ngenerated_response = agent.invoke(\n    { \"messages\": convert_messages(messages) },\n    { \"configurable\": { \"thread_id\": \"42\" } }\n)\n\nprint_full_response = False\n\nif (print_full_response):\n    print(generated_response)\nelse:\n    result = generated_response[\"messages\"][-1].content\n    print(f\"Agent: {result}\")\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Next steps\nYou successfully completed this notebook! You learned how to use\nwatsonx.ai inferencing SDK to generate response from the foundation model\nbased on the provided input, model id and model parameters. Check out the\nofficial watsonx.ai site for more samples, tutorials, documentation, how-tos, and blog posts.\n\n<a id=\"copyrights\"></a>\n### Copyrights\n\nLicensed Materials - Copyright \u00a9 2024 IBM. This notebook and its source code are released under the terms of the ILAN License.\nUse, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n\n**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for watsonx.ai Auto-generated Notebook (License Terms), such agreements located in the link below. Specifically, the Source Components and Sample Materials clause included in the License Information document for watsonx.ai Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n\nBy downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"https://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\" target=\"_blank\">License Terms</a>  "}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}}, "nbformat": 4, "nbformat_minor": 0}